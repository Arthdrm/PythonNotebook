{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1pVjrcBzZQ9gYjWvqdUlXT2tVoyuJE79W","authorship_tag":"ABX9TyPO2UooDAugs2xjfFI/OTYY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Initial Set Up ü¶¨\n","\n"],"metadata":{"id":"u60G640GTkgy"}},{"cell_type":"code","source":["####### Initial Set Up (Manual) #########\n","# Installing Java JDK on google colab\n","# JDK is needed because Spark is built on top of JVM. JDK provides neccesary tools to compile & run java codes and also to execute internal spark code.\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Download a prebuilt binary distribution of spark that compatible with hadoop\n","# Binary distribution -> Compiled code that can be executed directly by a computer's CPU\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n","\n","# Extract the tar file\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","\n","# Install the findspark package. It's used to locate the spark installation, so that PySpark can be used.\n","!pip install -q findspark\n","\n","# Set up the environment variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n","\n","# Import the libary\n","import findspark\n","\n","# Initiate findspark\n","findspark.init()\n","\n","# Check the location for Spark\n","findspark.find()\n","\n","# Import SparkSession\n","from pyspark.sql import SparkSession\n","\n","# Create a Spark Session\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Check Spark Session Information\n","spark"],"metadata":{"id":"k-gtca2OYcJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TDKIoe9DAk18","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"ok","timestamp":1685384856221,"user_tz":-480,"elapsed":61696,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"8973cdca-29e9-43c4-c791-05f4c4634250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=144838de5d634f5bf527e7cc568974af0599fb65227d602db41b31b56cdbd062\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7fb06743c220>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://6dc62e002f48:4050\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":1}],"source":["####### Initial Set Up (Automatic) #########\n","# Install pyspark\n","!pip install pyspark\n","# Import SparkSession\n","from pyspark.sql import SparkSession\n","from pyspark import SparkContext, SparkConf\n","conf = SparkConf().set('spark.ui.port', '4050')\n","sc = SparkContext(conf=conf)\n","# Create a Spark Session, sets up the local device as the master of the session\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","# Check Spark Session Information\n","spark"]},{"cell_type":"markdown","source":["# Dataframes Exploration üê≤\n"],"metadata":{"id":"nbn-H70UTxQA"}},{"cell_type":"code","source":["####### Creating a dataframe from a CSV file (With a header) #########\n","# Importing the sql types module that enables schema definitions.\n","from pyspark.sql.types import *\n","# Load a csv file to a dataframe (without a header)\n","df_citizen = spark.read.csv(\"/content/drive/MyDrive/SampleData/sampel_data_1.csv\")\n","print(\"Headerless dataframe : \")\n","# Print the content of a dataframe\n","df_citizen.show()\n","# Load a csv file to a dataframe (with a header)\n","df_citizen_header = spark.read.csv(\"/content/drive/MyDrive/SampleData/sampel_data_1.csv\", header=True)\n","print(\"Dataframe with header :\")\n","df_citizen_header.show()\n","print(\"Showing rows with name Andi :\")\n","# The headers of a dataframe become its properties\n","# The filter method is used to filter rows based on certain conditions.\n","df_citizen_header.filter(df_citizen_header.nama == \"andi\").show()"],"metadata":{"id":"rxhwVNm2Av2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### Creating a dataframe from a CSV file (With a predefined schema) #########\n","# Importing the sql types module that enables schema definitions.\n","from pyspark.sql.types import *\n","# Create a schema\n","schema_citizen = StructType([\n","    StructField('nama', StringType(), False),\n","    StructField('umur', IntegerType(), False)\n","])\n","# Load a csv file to a dataframe (with a schema)\n","df_citizen = spark.read.csv(\"/content/drive/MyDrive/SampleData/sampel_data_1.csv\", schema=schema_citizen)\n","# Print the content of a dataframe\n","df_citizen.show()\n"],"metadata":{"id":"gC2i43lgXtT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### Filtering the first row of a dataframe #########\n","from pyspark.sql.functions import monotonically_increasing_id\n","df_citizen_index = df_citizen.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n","df_citizen_index.filter(df_citizen_index.id > 0).show()"],"metadata":{"id":"g5Co5Fk5wAFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### Dataframe operation with domain-specific language #########\n","from pyspark.sql.types import *\n","schema_nilai_ipa = StructType([\n","    StructField(\"Nama\", StringType(), False),\n","    StructField(\"Kelas\", StringType(), False),\n","    StructField(\"Nilai\", IntegerType(), False)\n","])\n","nilai_ipa = [\n","    [\"Budi\", \"IX A\", 90],\n","    [\"Ando\", \"IX B\", 50],\n","    [\"Putu\", \"IX A\", 70],\n","    [\"Kadek\", \"IX C\", 40]\n","]\n","df_nilai_ipa = spark.createDataFrame(nilai_ipa, schema=schema_nilai_ipa)\n","print(\"Original Data :\")\n","df_nilai_ipa.show()\n","\n","# Adding a new column\n","df_nilai_ipa.withColumn(\"Nilai Up\", df_nilai_ipa[\"Nilai\"] + 10).show\n","# Select the first n-th rows from a dataframe. Returns a row object.\n","print(df_nilai_ipa.take(2))\n","# Selecting a specific field from a dataframe.\n","df_nilai_ipa.select(df_nilai_ipa.Kelas).show()"],"metadata":{"id":"mv9BvQq3B0n0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nilai_ipa = [\n","    [\"Budi\", \"IX A\", 90],\n","    [\"Ando\", \"IX B\", 50],\n","    [\"Putu\", \"IX A\", 70],\n","    [\"Kadek\", \"IX C\", 40]\n","]\n","df_nilai_ipa = spark.createDataFrame(nilai_ipa, schema=[\"Name\", \"Class\", \"Score\"])\n","print(\"Original Data :\")\n","df_nilai_ipa.show()\n","\n","# Adding a new column\n","df_new = df_nilai_ipa.withColumn(\"Nilai Up\", df_nilai_ipa[\"Score\"] + 10)\n","df_new.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnxbWxZFxVDn","executionInfo":{"status":"ok","timestamp":1685385055670,"user_tz":-480,"elapsed":1842,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"32de96c5-8ae4-462b-8257-42327a385822"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Data :\n","+-----+-----+-----+\n","| Name|Class|Score|\n","+-----+-----+-----+\n","| Budi| IX A|   90|\n","| Ando| IX B|   50|\n","| Putu| IX A|   70|\n","|Kadek| IX C|   40|\n","+-----+-----+-----+\n","\n","+-----+-----+-----+--------+\n","| Name|Class|Score|Nilai Up|\n","+-----+-----+-----+--------+\n","| Budi| IX A|   90|     100|\n","| Ando| IX B|   50|      60|\n","| Putu| IX A|   70|      80|\n","|Kadek| IX C|   40|      50|\n","+-----+-----+-----+--------+\n","\n"]}]},{"cell_type":"code","source":["# Importing all of the neccesary modules\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","\n","# Reading the raw data \n","df_singkatan = spark.read.csv(\"/content/drive/MyDrive/SampleData/dataset_singkatan_csv.csv\", header=True)\n","df_singkatan.show(truncate=False)"],"metadata":{"id":"6qSxTodH14ia","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685385334903,"user_tz":-480,"elapsed":3565,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"da664778-2c75-43ca-d359-caec214363dd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+--------------------+\n","|singkatan|provinsi            |\n","+---------+--------------------+\n","|sumut    |Sumatra Utara       |\n","|sumbar   |Sumatra Barat       |\n","|kepri    |Kepulauan Riau      |\n","|sumsel   |Sumatra Selatan     |\n","|babel    |Bangka Belitung     |\n","|jabar    |Jawa Barat          |\n","|jateng   |Jawa Tengah         |\n","|diy      |DI Yogyakarta       |\n","|jatim    |Jawa Timur          |\n","|ntb      |Nusa Tenggara Barat |\n","|ntt      |Nusa Tenggara Tinggi|\n","|kalbar   |Kalimantan Barat    |\n","|kalteng  |Kalimantan Tengah   |\n","|kalsel   |Kalimantan Selatan  |\n","|kaltim   |Kalimantan Timur    |\n","|kaltara  |Kalimantan Utara    |\n","|sulbar   |Sulawesi Barat      |\n","|sulsel   |Sulawesi Selatan    |\n","|sultra   |Sulawesi Tenggara   |\n","|sulteng  |Sulawesi Tengah     |\n","+---------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["####### Dataframe operation with SQL syntax #########\n","schema_nilai_ipa = StructType([\n","    StructField(\"Nama\", StringType(), False),\n","    StructField(\"Kelas\", StringType(), False),\n","    StructField(\"Nilai\", IntegerType(), False)\n","])\n","nilai_ipa = [\n","    [\"Budi\", \"IX A\", 90],\n","    [\"Putu\", \"IX A\", 70],\n","    [\"Ando\", \"IX B\", 50],\n","    [\"Kadek\", \"IX C\", 40]\n","]\n","df_nilai_ipa = spark.createDataFrame(nilai_ipa, schema=schema_nilai_ipa)\n","print(\"Data original :\")\n","df_nilai_ipa.show()\n","\n","# Memfilter data (Menggunakan API)\n","print(\"Menampilkan data siswa kelas IX A : \")\n","df_nilai_ipa.filter(df_nilai_ipa[\"Kelas\"] == \"IX A\").show()\n","\n","# Memfilter data (Menggunakan Query SQL)\n","# First, we have to create a temporary view (virtual table limited to the current session)\n","df_nilai_ipa.createOrReplaceTempView(\"tb_nilai_ipa\")\n","# Then, we could use the temporary view we've created as a target for our queries.\n","# Remember a view doesn't hold any actual data as it's not a physical table.\n","# The sql() method returns a dataframe. That's why we can use the show() method on it.\n","print(\"Menampilkan data siswa kelas IX C : \")\n","spark.sql('SELECT * from tb_nilai_ipa WHERE Kelas = \"IX C\"').show()"],"metadata":{"id":"HwNcdq_SP3V0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685385342000,"user_tz":-480,"elapsed":2629,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"c8e90163-192e-4632-b475-f89d78d5736a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Data original :\n","+-----+-----+-----+\n","| Nama|Kelas|Nilai|\n","+-----+-----+-----+\n","| Budi| IX A|   90|\n","| Putu| IX A|   70|\n","| Ando| IX B|   50|\n","|Kadek| IX C|   40|\n","+-----+-----+-----+\n","\n","Menampilkan data siswa kelas IX A : \n","+----+-----+-----+\n","|Nama|Kelas|Nilai|\n","+----+-----+-----+\n","|Budi| IX A|   90|\n","|Putu| IX A|   70|\n","+----+-----+-----+\n","\n","Menampilkan data siswa kelas IX C : \n","+-----+-----+-----+\n","| Nama|Kelas|Nilai|\n","+-----+-----+-----+\n","|Kadek| IX C|   40|\n","+-----+-----+-----+\n","\n"]}]},{"cell_type":"code","source":["# Mengimpor modul-modul yang diperlukan\n","from pyspark.sql.types import *\n","from pyspark.sql import SparkSession\n","# Menginstansiasi objek SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","# Membuat skema untuk dataframe\n","schema_nilai_ipa = StructType([\n"," StructField(\"Nama\", StringType(), False),\n"," StructField(\"Kelas\", StringType(), False),\n"," StructField(\"Nilai\", IntegerType(), False)\n","])\n","# Membuat nested list yang nantinya akan di-load ke dataframe\n","nilai_ipa = [\n"," [\"Budi\", \"IX A\", 90],\n"," [\"Putu\", \"IX A\", 70],\n"," [\"Ando\", \"IX B\", 50],\n"," [\"Kadek\", \"IX C\", 40]\n","]\n","# Membuat dataframe menggunakan metode createDataFrame()\n","df_nilai_ipa = spark.createDataFrame(nilai_ipa, schema=schema_nilai_ipa)\n","# Menampilkan isi dataframe menggunakan metode show()\n","print(\"Data Original :\")\n","df_nilai_ipa.show()\n","# Memfilter data (Menggunakan API)\n","print(\"Menampilkan data siswa kelas IX A : \")\n","df_nilai_ipa.filter(df_nilai_ipa[\"Kelas\"] == \"IX A\").show()\n","# Memfilter data (Menggunakan Query SQL)\n","# Pertama-tama kita harus membuat sebuah temporary view sebagai target query SQL.\n","df_nilai_ipa.createOrReplaceTempView(\"tb_nilai_ipa\")\n","# Selanjutnya, kita dapat menggunakan metode sql() untuk menjalankan query SQL pada temporary view yang telah dibuat.\n","print(\"Menampilkan data siswa kelas IX C : \")\n","spark.sql('SELECT * from tb_nilai_ipa WHERE Kelas = \"IX C\"').show()"],"metadata":{"id":"OhYYS451MxvJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RDDs Exploration üêÑ"],"metadata":{"id":"05U9bBWbUBaf"}},{"cell_type":"code","source":["####### Creating an RDD #########\n","# Setting up the configuration for SparkContext\n","from pyspark import SparkContext, SparkConf\n","spark_conf = SparkConf()\\\n","  .setAppName(\"YourTest\")\\\n","  .setMaster(\"local[*]\")\n","sc = SparkContext.getOrCreate(spark_conf)\n","# Creating a list that contains number from 1 - 100.000\n","num = list(range(1, 100000))\n","# Creating an RDD from a list using the sc.parallelize() method\n","num_rdd = sc.parallelize(num)\n","\n","num_rdd_1 = num_rdd.map(lambda x: x * 2)\n","# Checking whether the RDD is succesfully created.\n","num_rdd_1.collect()"],"metadata":{"id":"TvsNvAZwKUVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying a mapping function to each element of num_rdd\n","quadrupled_num_rdd = num_rdd.map(lambda x: x * 4)\n","# Showing the content of num_rdd using the rdd.collect() method.\n","quadrupled_num_rdd.collect()\n"],"metadata":{"id":"h6Cu9EcNLPP9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679583693915,"user_tz":-480,"elapsed":515,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"89628971-e4dc-486b-fe74-2df7e0423a8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 8, 12, 16, 20, 24, 28, 32, 36]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"R7z7s9FHtcdu"},"execution_count":null,"outputs":[]}]}