{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+f1f2po+CNCxFUVUrtLyK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install keras_preprocessing"],"metadata":{"id":"KjlX1TZ504FZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711772648415,"user_tz":-480,"elapsed":21321,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"4ebef091-f70d-42e8-905f-36dfcf140c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras_preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m568.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.25.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n","Installing collected packages: keras_preprocessing\n","Successfully installed keras_preprocessing-1.1.2\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"xrCYPCYnkcj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"],"metadata":{"id":"Eqd36yStmkfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test = imdb['train'], imdb['test']"],"metadata":{"id":"CPnbMKD0pr0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices(\n","    [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","for x in dataset:\n","  print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7a_wCvfI4lE","executionInfo":{"status":"ok","timestamp":1711815269830,"user_tz":-480,"elapsed":521,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"45bccddf-026d-487a-ae41-3a1082694ac4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n","tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n","tf.Tensor([7 8 9], shape=(3,), dtype=int32)\n"]}]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices(\n","    [[[1,2], [1,2], [1,2]], [[1,2], [1,2], [1,2]], [[1,2], [1,2], [1,2]]])\n","for x in dataset:\n","  print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lprlwfWJHnui","executionInfo":{"status":"ok","timestamp":1711815427273,"user_tz":-480,"elapsed":550,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"6e6b96af-e606-4c3d-e571-7e205235ef46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[1 2]\n"," [1 2]\n"," [1 2]], shape=(3, 2), dtype=int32)\n","tf.Tensor(\n","[[1 2]\n"," [1 2]\n"," [1 2]], shape=(3, 2), dtype=int32)\n","tf.Tensor(\n","[[1 2]\n"," [1 2]\n"," [1 2]], shape=(3, 2), dtype=int32)\n"]}]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices([1,2,3])\n","print(dataset)\n","for x in dataset:\n","  print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_Is33RCHwNN","executionInfo":{"status":"ok","timestamp":1711814972681,"user_tz":-480,"elapsed":568,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"4fb72be8-61a5-4152-d94d-edb41178e3eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n","tf.Tensor(1, shape=(), dtype=int32)\n","tf.Tensor(2, shape=(), dtype=int32)\n","tf.Tensor(3, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["class MyCallback(tf.keras.callbacks.Callback):\n","\tdef on_epoch_end(self, epoch, logs={}):\n","\t\tif logs.get(\"loss\") < 1e-4:\n","\t\t\tself.model.stop_training = True"],"metadata":{"id":"NtyfYMDl4FuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PReKzXBYBqi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702807448012,"user_tz":-480,"elapsed":1176350,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"b8a085e8-e142-47eb-afd8-0241e6bfe95e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/500\n","1349/1349 [==============================] - 62s 40ms/step - loss: 0.0492 - mae: 0.0492 - val_loss: 0.0486 - val_mae: 0.0486\n","Epoch 2/500\n","1349/1349 [==============================] - 48s 36ms/step - loss: 0.0442 - mae: 0.0442 - val_loss: 0.0483 - val_mae: 0.0483\n","Epoch 3/500\n","1349/1349 [==============================] - 47s 35ms/step - loss: 0.0437 - mae: 0.0437 - val_loss: 0.0475 - val_mae: 0.0475\n","Epoch 4/500\n","1349/1349 [==============================] - 47s 35ms/step - loss: 0.0434 - mae: 0.0434 - val_loss: 0.0477 - val_mae: 0.0477\n","Epoch 5/500\n","1349/1349 [==============================] - 48s 35ms/step - loss: 0.0431 - mae: 0.0431 - val_loss: 0.0471 - val_mae: 0.0471\n","Epoch 6/500\n","1349/1349 [==============================] - 47s 34ms/step - loss: 0.0429 - mae: 0.0429 - val_loss: 0.0472 - val_mae: 0.0472\n","Epoch 7/500\n","1349/1349 [==============================] - 55s 41ms/step - loss: 0.0427 - mae: 0.0427 - val_loss: 0.0478 - val_mae: 0.0478\n","Epoch 8/500\n","1349/1349 [==============================] - 48s 35ms/step - loss: 0.0423 - mae: 0.0423 - val_loss: 0.0476 - val_mae: 0.0476\n","Epoch 9/500\n","1349/1349 [==============================] - 46s 34ms/step - loss: 0.0421 - mae: 0.0421 - val_loss: 0.0478 - val_mae: 0.0478\n","Epoch 10/500\n","1349/1349 [==============================] - 48s 36ms/step - loss: 0.0417 - mae: 0.0417 - val_loss: 0.0482 - val_mae: 0.0482\n","Epoch 11/500\n","1349/1349 [==============================] - 47s 34ms/step - loss: 0.0418 - mae: 0.0418 - val_loss: 0.0486 - val_mae: 0.0486\n","Epoch 12/500\n","1349/1349 [==============================] - 53s 40ms/step - loss: 0.0413 - mae: 0.0413 - val_loss: 0.0480 - val_mae: 0.0480\n","Epoch 13/500\n","1349/1349 [==============================] - 46s 34ms/step - loss: 0.0410 - mae: 0.0410 - val_loss: 0.0483 - val_mae: 0.0483\n","Epoch 14/500\n","1349/1349 [==============================] - 48s 35ms/step - loss: 0.0408 - mae: 0.0408 - val_loss: 0.0495 - val_mae: 0.0495\n","Epoch 15/500\n","1349/1349 [==============================] - 50s 37ms/step - loss: 0.0405 - mae: 0.0405 - val_loss: 0.0494 - val_mae: 0.0494\n","Epoch 16/500\n","1349/1349 [==============================] - 47s 35ms/step - loss: 0.0403 - mae: 0.0403 - val_loss: 0.0491 - val_mae: 0.0491\n","Epoch 17/500\n","1349/1349 [==============================] - 47s 35ms/step - loss: 0.0402 - mae: 0.0402 - val_loss: 0.0487 - val_mae: 0.0487\n","Epoch 18/500\n","1349/1349 [==============================] - 54s 40ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0488 - val_mae: 0.0488\n","Epoch 19/500\n","1349/1349 [==============================] - 54s 40ms/step - loss: 0.0397 - mae: 0.0397 - val_loss: 0.0493 - val_mae: 0.0493\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# ============================================================================================\n","# PROBLEM C5\n","#\n","# Build and train a neural network to predict time indexed variables of\n","# the multivariate house hold electric power consumption time series dataset.\n","# Using a window of past 24 observations of the 7 variables, the model\n","# should be trained to predict the next 24 observations of the 7 variables.\n","# Use MAE as the metrics of your neural network model.\n","# We provided code for normalizing the data. Please do not change the code.\n","# Do not use lambda layers in your model.\n","#\n","# The dataset used in this problem is downloaded from https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption\n","#\n","# Desired MAE < 0.1 on the normalized dataset.\n","# ============================================================================================\n","\n","import urllib\n","import os\n","import zipfile\n","import pandas as pd\n","import tensorflow as tf\n","\n","# This function downloads and extracts the dataset to the directory that contains this file.\n","# DO NOT CHANGE THIS CODE\n","# (unless you need to change the URL)\n","def download_and_extract_data():\n","    url = 'https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/household_power.zip'\n","    urllib.request.urlretrieve(url, 'household_power.zip')\n","    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n","        zip_ref.extractall()\n","\n","class MyCallback(tf.keras.callbacks.Callback):\n","\tdef on_epoch_end(self, epoch, logs={}):\n","\t\tif logs.get(\"mae\") < 0.04:\n","\t\t\tself.model.stop_training = True\n","\n","# This function normalizes the dataset using min max scaling.\n","# DO NOT CHANGE THIS CODE\n","def normalize_series(data, min, max):\n","    data = data - min\n","    data = data / max\n","    return data\n","\n","# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n","def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(n_past+n_future, shift=shift, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(n_past+n_future))\n","    ds = ds.shuffle(1000)\n","    ds = ds.map(lambda w: (w[:-n_past], w[-n_past:, :1]))\n","    return ds.batch(batch_size).prefetch(1)\n","\n","# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n","def solution_C5():\n","    # Downloads and extracts the dataset to the directory that contains this file.\n","    download_and_extract_data()\n","    # Reads the dataset from the csv.\n","    df = pd.read_csv('household_power_consumption.csv', sep=',',\n","                     infer_datetime_format=True, index_col='datetime', header=0)\n","\n","    # Number of features in the dataset. We use all features as predictors to\n","    # predict all features at future time steps.\n","    N_FEATURES = len(df.columns)\n","\n","    # Normalizes the data\n","    # DO NOT CHANGE THIS\n","    data = df.values\n","    split_time = int(len(data) * 0.5)\n","    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n","\n","    # Splits the data into training and validation sets.\n","    x_train = data[:split_time]\n","    x_valid = data[split_time:]\n","\n","    # DO NOT CHANGE THIS\n","    BATCH_SIZE = 32\n","    N_PAST = 24 # Number of past time steps based on which future observations should be predicted\n","    N_FUTURE = 24  # Number of future time steps which are to be predicted.\n","    SHIFT = 1  # By how many positions the window slides to create a new window of observations.\n","\n","    # Code to create windowed train and validation datasets.\n","    # Complete the code in windowed_dataset.\n","    train_set = windowed_dataset(x_train, BATCH_SIZE)\n","    valid_set = windowed_dataset(x_valid, BATCH_SIZE)\n","\n","    # Code to define your model.\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=[N_PAST, N_FEATURES]),\n","        tf.keras.layers.LSTM(32, return_sequences=True),\n","        tf.keras.layers.LSTM(32),\n","        tf.keras.layers.Dense(64, activation=\"relu\"),\n","        tf.keras.layers.Dense(64, activation=\"relu\"),\n","        tf.keras.layers.Dense(N_FUTURE * N_FEATURES),\n","        tf.keras.layers.Reshape((N_FUTURE, N_FEATURES))\n","    ])\n","\n","    # Code to train and compile the model\n","    # YOUR CODE HERE\n","    callback = MyCallback()\n","    model.compile(\n","        loss='mae',\n","        optimizer=\"adam\",\n","        metrics=['mae']\n","    )\n","\n","    model.fit(train_set, validation_data=valid_set, epochs=500, callbacks=[callback])\n","\n","    return model\n","\n","# The code below is to save your model as a .h5 file.\n","# It will be saved automatically in your Submission folder.\n","if __name__ == '__main__':\n","    # DO NOT CHANGE THIS CODE\n","    model = solution_C5()\n","    model.save(\"model_C5.h5\")"]},{"cell_type":"code","source":["training_labels.head(10)"],"metadata":{"id":"oTt-c1HmzpZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text='''\n","gamers snap up new sony psp gamers have bought almost all of the first batch of sony s new playstation portable (psp) games console  which went on sale in japan on sunday.  thousands of people queued for hours to get hold of one of the 200 000 psps which were shipped to retailers. the handheld console can play games  music and movies and goes on sale in europe and north america next year. despite the demand sony said it would not increase the 500 000-strong stock of psps it plans to ship by year s end.  sony says it intends to ship three million of the consoles by march 2005. the company is hoping to challenge the dominance of nintendo in the handheld market. nintendo released its new ds console earlier this year and has already raised shipment targets for the device by 40%. the psp is selling in japan for 19 800 yen ($188; £98) while nintendo s ds console sells in the us and japan for $150 (£78). nintendo s goal is to ship 5 million of its new nintendo ds handheld consoles by march 2005.\n","'''"],"metadata":{"id":"mTEJ6xBEyWa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer used during training\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text)\n","\n","# Tokenize and pad the input text\n","sequence = tokenizer.texts_to_sequences(text)\n","padded_sequence = pad_sequences(sequence, maxlen=120, padding='post', truncating='post')\n","\n","# Make predictions\n","predictions = model.predict(padded_sequence)\n","\n","# Print the predicted class probabilities\n","print(\"Predicted Probabilities:\", predictions)\n","\n","# Get the predicted class (index with maximum probability)\n","predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n","print(predicted_class)\n","\n","# Assuming you have a mapping from index to class label\n","class_mapping = {0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}\n","\n","# Print the predicted class label\n","print(\"Predicted Class:\", class_mapping[predicted_class])"],"metadata":{"id":"K_pEx3vRC54y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702649245441,"user_tz":-480,"elapsed":507,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"d82e1771-c3bd-481f-8fad-83063fb3a1bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 0s 2ms/step\n","Predicted Probabilities: [[2.0964621e-06 8.3767780e-04 9.9879324e-01 2.0241036e-05 3.4673919e-04]\n"," [2.3654216e-06 8.2174223e-04 9.9876404e-01 2.1848624e-05 3.8992715e-04]\n"," [2.1903588e-06 8.2334212e-04 9.9878317e-01 2.1547898e-05 3.6970360e-04]\n"," ...\n"," [2.4722492e-06 8.9983398e-04 9.9870205e-01 2.1365899e-05 3.7435879e-04]\n"," [2.0964601e-06 8.3767739e-04 9.9879324e-01 2.0240997e-05 3.4673905e-04]\n"," [2.0964601e-06 8.3767739e-04 9.9879324e-01 2.0240997e-05 3.4673905e-04]]\n","2\n","Predicted Class: politics\n"]}]},{"cell_type":"code","source":["print(model.layers[0].input_shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWufBws8VeHX","executionInfo":{"status":"ok","timestamp":1702634335597,"user_tz":-480,"elapsed":335,"user":{"displayName":"arth d","userId":"18418989980058109684"}},"outputId":"febf19db-8fb2-43ca-d608-c36702a3a4ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1)\n"]}]}]}